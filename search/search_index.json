{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OSG Collaborations Support \u00b6 Mission \u00b6 The mission of OSG Collaboration Support is to enable access and lower the barrier for Midsize collaborations to leverage the the distributed High Throughput Computing (dHTC) ecosystem of OSG for their research. Collaboration Institutions and Compute Sites \u00b6 Contact Us \u00b6 Open a Ticket by sending an email to help@opensciencegrid.org and request support for your collaboration project. Regular Meeting schedule \u00b6 Collaboration support holds and participates in meetings during the week to assist and coordinate on projects. A list of the weekly meeting schedule is maintained here List of supported collaboratoins \u00b6 Collaboration list","title":"Home"},{"location":"#osg-collaborations-support","text":"","title":"OSG Collaborations Support"},{"location":"#mission","text":"The mission of OSG Collaboration Support is to enable access and lower the barrier for Midsize collaborations to leverage the the distributed High Throughput Computing (dHTC) ecosystem of OSG for their research.","title":"Mission"},{"location":"#collaboration-institutions-and-compute-sites","text":"","title":"Collaboration Institutions and Compute Sites"},{"location":"#contact-us","text":"Open a Ticket by sending an email to help@opensciencegrid.org and request support for your collaboration project.","title":"Contact Us"},{"location":"#regular-meeting-schedule","text":"Collaboration support holds and participates in meetings during the week to assist and coordinate on projects. A list of the weekly meeting schedule is maintained here","title":"Regular Meeting schedule"},{"location":"#list-of-supported-collaboratoins","text":"Collaboration list","title":"List of supported collaboratoins"},{"location":"misc/meeting-schedule/","text":"Schedule of collaborative meetings/calls \u00b6 OSG Area Coordinators meeting \u00b6 When: Wednesdays 2-3pm Central URL: https://unl.zoom.us/j/865025296 IGWN-CESER meetting \u00b6 When: Tuesdays 12:30pm-1:30pm Central Details in the IGWN Collaboration page Icecube-CESER meeting \u00b6 When: Biweeklyy on Mondays 3:00 pm-4:00pm Central Detais in the Icecube Collaboration page XENON Analysis/Compute meeting \u00b6 When: Thursdays 10:00am-11:00am Central Details in the XENON Collaboration page IGWN production meeting \u00b6 When: Thursdays 9:00am-10:00am Central Details in the IGWN Collaboration page Koto support meetting \u00b6 When: Thursdays 3:00pm - 4:00pm Central URL: https://uchicago.zoom.us/s/4250591437 xCache meetting \u00b6 When: Thursdays 1:00pm-2:00pm Central URL: https://unl.zoom.us/j/651969661 Internet2-CESER meeting \u00b6 When: Fridays 2:00pm-3:00pm Central URL: https://internet2.zoom.us/j/668736847","title":"Meeting/Call schedule"},{"location":"misc/meeting-schedule/#schedule-of-collaborative-meetingscalls","text":"","title":"Schedule of collaborative meetings/calls"},{"location":"misc/meeting-schedule/#osg-area-coordinators-meeting","text":"When: Wednesdays 2-3pm Central URL: https://unl.zoom.us/j/865025296","title":"OSG Area Coordinators meeting"},{"location":"misc/meeting-schedule/#igwn-ceser-meetting","text":"When: Tuesdays 12:30pm-1:30pm Central Details in the IGWN Collaboration page","title":"IGWN-CESER meetting"},{"location":"misc/meeting-schedule/#icecube-ceser-meeting","text":"When: Biweeklyy on Mondays 3:00 pm-4:00pm Central Detais in the Icecube Collaboration page","title":"Icecube-CESER meeting"},{"location":"misc/meeting-schedule/#xenon-analysiscompute-meeting","text":"When: Thursdays 10:00am-11:00am Central Details in the XENON Collaboration page","title":"XENON Analysis/Compute meeting"},{"location":"misc/meeting-schedule/#igwn-production-meeting","text":"When: Thursdays 9:00am-10:00am Central Details in the IGWN Collaboration page","title":"IGWN production meeting"},{"location":"misc/meeting-schedule/#koto-support-meetting","text":"When: Thursdays 3:00pm - 4:00pm Central URL: https://uchicago.zoom.us/s/4250591437","title":"Koto support meetting"},{"location":"misc/meeting-schedule/#xcache-meetting","text":"When: Thursdays 1:00pm-2:00pm Central URL: https://unl.zoom.us/j/651969661","title":"xCache meetting"},{"location":"misc/meeting-schedule/#internet2-ceser-meeting","text":"When: Fridays 2:00pm-3:00pm Central URL: https://internet2.zoom.us/j/668736847","title":"Internet2-CESER meeting"},{"location":"projects/Redtop/","text":"REDTOP \u00b6 Support areas of REDTOP include data management, provisioned storage on Stash (~100 TB), and assistance in executing their job workloads. Computing Model for REDTOP using OSG resources: https://redtop.fnal.gov/computing-model/ OSG Collaboration Support Services supports REDTOP workflows on the OpenScienceGrid and has provided a compute plan in support of the colllaboration's LOI to Snowmass21","title":"REDTOP"},{"location":"projects/Redtop/#redtop","text":"Support areas of REDTOP include data management, provisioned storage on Stash (~100 TB), and assistance in executing their job workloads. Computing Model for REDTOP using OSG resources: https://redtop.fnal.gov/computing-model/ OSG Collaboration Support Services supports REDTOP workflows on the OpenScienceGrid and has provided a compute plan in support of the colllaboration's LOI to Snowmass21","title":"REDTOP"},{"location":"projects/clas12/","text":"CLAS12 \u00b6 The CLAS12 experiment at Jefferson Lab is supported in part by the U.S. Department of Energy, the National Science Foundation (NSF), the Italian Istituto Nazionale di Fisica Nucleare (INFN), the French Centre National de la Recherche Scientifique (CNRS), the French Commissariat pour l\u2019Energie Atomique, the UK Science and Technology Facilities Council and other international funding CLAS12 data and production management: Maurizio Ungaro ungaro@jlab.org","title":"GLAS12"},{"location":"projects/clas12/#clas12","text":"The CLAS12 experiment at Jefferson Lab is supported in part by the U.S. Department of Energy, the National Science Foundation (NSF), the Italian Istituto Nazionale di Fisica Nucleare (INFN), the French Centre National de la Recherche Scientifique (CNRS), the French Commissariat pour l\u2019Energie Atomique, the UK Science and Technology Facilities Council and other international funding CLAS12 data and production management: Maurizio Ungaro ungaro@jlab.org","title":"CLAS12"},{"location":"projects/eic/","text":"EIC \u00b6 EIC (Electron-Ion Collider) is a proposed High Energy Physics experiment, a joint effort between Brookhaven and Jefferson national labs. Overall project management lead: Jerome Lauret eromel@bnl.gov BNL project coordinator: Xin Zhao xzhao@bnl.gov JLAB project coordinators: Markus Diefenthaler, mdiefent@jlab.org and David Lawrence, davidl@jlab.org","title":"EIC"},{"location":"projects/eic/#eic","text":"EIC (Electron-Ion Collider) is a proposed High Energy Physics experiment, a joint effort between Brookhaven and Jefferson national labs. Overall project management lead: Jerome Lauret eromel@bnl.gov BNL project coordinator: Xin Zhao xzhao@bnl.gov JLAB project coordinators: Markus Diefenthaler, mdiefent@jlab.org and David Lawrence, davidl@jlab.org","title":"EIC"},{"location":"projects/gluex/","text":"GLUEX \u00b6 The GlueX experiment at Jefferson Lab is supported in part by the U.S. Department of Energy, the U.S. National Science Foundation, the German Research Foundation, and other international foundations and institutions. The UConn group acknowledges support for work on GlueX from the U.S. National Science Foundation under grant 1812415. GlueX data and production management: Richard Jones richard.t.jones@uconn.edu JLAB GlueX coordinators: Thomas Britton tbritton@jlab.org and Bryan Hess bhess@jlab.org","title":"GLUEX"},{"location":"projects/gluex/#gluex","text":"The GlueX experiment at Jefferson Lab is supported in part by the U.S. Department of Energy, the U.S. National Science Foundation, the German Research Foundation, and other international foundations and institutions. The UConn group acknowledges support for work on GlueX from the U.S. National Science Foundation under grant 1812415. GlueX data and production management: Richard Jones richard.t.jones@uconn.edu JLAB GlueX coordinators: Thomas Britton tbritton@jlab.org and Bryan Hess bhess@jlab.org","title":"GLUEX"},{"location":"projects/icecube/","text":"IceCube-PATh \u00b6 Icecube is supported by OSG Collaborations via the PATh project. This page will include meeting-minutes and location along with status reports","title":"IceCube"},{"location":"projects/icecube/#icecube-path","text":"Icecube is supported by OSG Collaborations via the PATh project. This page will include meeting-minutes and location along with status reports","title":"IceCube-PATh"},{"location":"projects/igwn/","text":"International Gravitational-Wave Observatory Network \u00b6 Support areas for IGWN include data management, infrastructure (StashCaches, FTS service for Rucio managed third-party transfers), best practices for software and job management, and in-person participation in the Open Science - LIGO weekly meetings. The LIGO component of the IGWN collaboration has 119 institutions and universities in the US that make use of the Open Science distributed infrastructure for computing. LIGO is supported by the NSF award # PHY-2110594 for the LIGO Data Grid (LDG) and NSF award # PHY-1764464 for the LIGO Lab in computing, R&D, and operations. A detailed list of technical contacts and leads for the various workloads is maintained by the LIGO and Open Science group here. For general purpose data and job management, the lead is: James Clark james.clark@ligo.org Institutions Compute Resources Weekly IGWN-PATh meeting \u00b6 Topic: PATh-IGWN technical call Time: 11:00 PM Central Time (US and Canada) Every week on Thur Join Zoom Meeting: TB-updated Weekly IGWN Production meeting \u00b6 Time: 9:00 AM Central Time (US and Canada) Every Week on Thu Links to be updated. Currently access is only restricted to people with ligo credentials. Weekly IGWN Condor meeting \u00b6 TB updated","title":"IGWN"},{"location":"projects/igwn/#international-gravitational-wave-observatory-network","text":"Support areas for IGWN include data management, infrastructure (StashCaches, FTS service for Rucio managed third-party transfers), best practices for software and job management, and in-person participation in the Open Science - LIGO weekly meetings. The LIGO component of the IGWN collaboration has 119 institutions and universities in the US that make use of the Open Science distributed infrastructure for computing. LIGO is supported by the NSF award # PHY-2110594 for the LIGO Data Grid (LDG) and NSF award # PHY-1764464 for the LIGO Lab in computing, R&D, and operations. A detailed list of technical contacts and leads for the various workloads is maintained by the LIGO and Open Science group here. For general purpose data and job management, the lead is: James Clark james.clark@ligo.org Institutions Compute Resources","title":"International Gravitational-Wave Observatory Network"},{"location":"projects/igwn/#weekly-igwn-path-meeting","text":"Topic: PATh-IGWN technical call Time: 11:00 PM Central Time (US and Canada) Every week on Thur Join Zoom Meeting: TB-updated","title":"Weekly IGWN-PATh meeting"},{"location":"projects/igwn/#weekly-igwn-production-meeting","text":"Time: 9:00 AM Central Time (US and Canada) Every Week on Thu Links to be updated. Currently access is only restricted to people with ligo credentials.","title":"Weekly IGWN Production meeting"},{"location":"projects/igwn/#weekly-igwn-condor-meeting","text":"TB updated","title":"Weekly IGWN Condor meeting"},{"location":"projects/koto/","text":"KOTO \u00b6 KOTO is an international collaboration of an experiment based at the JPARC HEP facility in Japan. Access Point \u00b6 KOTO submits out of the Connect node: login.collab.ci-connect.net. To request an account, visit the portal at https://ci-connect.net and ask to join the PATh Collaborations group. KOTO is a subgroup under PATh collaborations and a separate request needs to be made to join by clicking the request memberhip button. Jobs to the OSPool \u00b6 Inlined below is a submit script to run a KOTO job on the OSPool 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash Universe = Vanilla Executable = run_koto.sh Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/ppaschos/koto-dev:latest\" transfer_input_files = e14_201605.mac Error = output.err. $( Cluster ) - $( Process ) Output = output.out. $( Cluster ) - $( Process ) Log = output.log. $( Cluster ) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 2GB request_disk = 2GB +ProjectName = \"collab.KOTO\" Queue 1 KOTO uses a software stack that is included in the Singularity image - shown in the script above. The image is deployed on the remote execution nodes and jobs run within the Singularity container. The input file, e14_201605.mac, is sent to the execution node as part of the job. The execution script, run_koto.sh, sets the environment and includes all invocations to executables. The following script, run_koto.sh, runs a benchmark example from the E14 library. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/bash echo $HOSTNAME source /opt/koto/root/v6.22.02/bin/thisroot.sh source /opt/koto/geant4/10.05.p01/bin/geant4.sh echo \"PATH --> \" $PATH echo \"LD_LIBRARY_PATH --> \" $LD_LIBRARY_PATH echo \"ROOTSYS --> \" $ROOTSYS echo \"G4 data environent\" env | grep G4 export E14_TOP_DIR = \"/opt/koto/e14/201605v6.2/e14\" echo \"E14_TOP_DIR=\" $E14_TOP_DIR source /opt/koto/e14/201605v6.2/setup.sh output = \"output.root\" nevent = 1000 seed = 0 echo \"Running Code\" /opt/koto/e14/201605v6.2/e14/examples/gsim4test/bin/gsim4test e14_201605.mac ${ output } ${ nevent } ${ seed } ${ seed } Storage access \u00b6 Besides their home directories on login.collab.ci-connect.net, users have access to local storage in /scratch/ and distributed storage in /collab/user/ . In addition, the collaboration has access to /collab/project/KOTO for sharing data products between the users. The distributed storage is http accessible: http://stash.osgstorage.org/collab/KOTO and data can be downloaded using a browser or command line utilities like wget or curl.","title":"KOTO"},{"location":"projects/koto/#koto","text":"KOTO is an international collaboration of an experiment based at the JPARC HEP facility in Japan.","title":"KOTO"},{"location":"projects/koto/#access-point","text":"KOTO submits out of the Connect node: login.collab.ci-connect.net. To request an account, visit the portal at https://ci-connect.net and ask to join the PATh Collaborations group. KOTO is a subgroup under PATh collaborations and a separate request needs to be made to join by clicking the request memberhip button.","title":"Access Point"},{"location":"projects/koto/#jobs-to-the-ospool","text":"Inlined below is a submit script to run a KOTO job on the OSPool 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #!/bin/bash Universe = Vanilla Executable = run_koto.sh Requirements = HAS_SINGULARITY == TRUE +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/ppaschos/koto-dev:latest\" transfer_input_files = e14_201605.mac Error = output.err. $( Cluster ) - $( Process ) Output = output.out. $( Cluster ) - $( Process ) Log = output.log. $( Cluster ) should_transfer_files = YES WhenToTransferOutput = ON_EXIT request_cpus = 1 request_memory = 2GB request_disk = 2GB +ProjectName = \"collab.KOTO\" Queue 1 KOTO uses a software stack that is included in the Singularity image - shown in the script above. The image is deployed on the remote execution nodes and jobs run within the Singularity container. The input file, e14_201605.mac, is sent to the execution node as part of the job. The execution script, run_koto.sh, sets the environment and includes all invocations to executables. The following script, run_koto.sh, runs a benchmark example from the E14 library. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/bash echo $HOSTNAME source /opt/koto/root/v6.22.02/bin/thisroot.sh source /opt/koto/geant4/10.05.p01/bin/geant4.sh echo \"PATH --> \" $PATH echo \"LD_LIBRARY_PATH --> \" $LD_LIBRARY_PATH echo \"ROOTSYS --> \" $ROOTSYS echo \"G4 data environent\" env | grep G4 export E14_TOP_DIR = \"/opt/koto/e14/201605v6.2/e14\" echo \"E14_TOP_DIR=\" $E14_TOP_DIR source /opt/koto/e14/201605v6.2/setup.sh output = \"output.root\" nevent = 1000 seed = 0 echo \"Running Code\" /opt/koto/e14/201605v6.2/e14/examples/gsim4test/bin/gsim4test e14_201605.mac ${ output } ${ nevent } ${ seed } ${ seed }","title":"Jobs to the OSPool"},{"location":"projects/koto/#storage-access","text":"Besides their home directories on login.collab.ci-connect.net, users have access to local storage in /scratch/ and distributed storage in /collab/user/ . In addition, the collaboration has access to /collab/project/KOTO for sharing data products between the users. The distributed storage is http accessible: http://stash.osgstorage.org/collab/KOTO and data can be downloaded using a browser or command line utilities like wget or curl.","title":"Storage access"},{"location":"projects/lsst/","text":"LSST via Duke \u00b6 LSST is supported by OSG Collaboration Services via the duke.lsst osg project Sign up to get access to the OSG submit node here: Duke CI-Connect Access Portal News \u00b6 You can now use stashcp to transfer data to and from OSG as part of your computational workflow. You must transfer your input files to a new namespace. From your active session on the duke login node the namespace is located in: /collab/users and /collab/project/lsst Users can create their own private directories in users or shared directories in lsst. New accounts will create the user directories automatically and provide read,write permissions in the project/lsst folder To transfer files to a remote site via a job enter this stashcp command in your execution script: stashcp /osgconnect/collab/users/ / . To transfer files, such as output, from the remote site where your jobs is running to your space on stash, enter this command in your execution script stashcp stash:///osgconnect/collab/users/ /.","title":"LSST"},{"location":"projects/lsst/#lsst-via-duke","text":"LSST is supported by OSG Collaboration Services via the duke.lsst osg project Sign up to get access to the OSG submit node here: Duke CI-Connect Access Portal","title":"LSST via Duke"},{"location":"projects/lsst/#news","text":"You can now use stashcp to transfer data to and from OSG as part of your computational workflow. You must transfer your input files to a new namespace. From your active session on the duke login node the namespace is located in: /collab/users and /collab/project/lsst Users can create their own private directories in users or shared directories in lsst. New accounts will create the user directories automatically and provide read,write permissions in the project/lsst folder To transfer files to a remote site via a job enter this stashcp command in your execution script: stashcp /osgconnect/collab/users/ / . To transfer files, such as output, from the remote site where your jobs is running to your space on stash, enter this command in your execution script stashcp stash:///osgconnect/collab/users/ /.","title":"News"},{"location":"projects/moller/","text":"MOLLER \u00b6 The Measurement of a Lepton-Lepton Electroweak Reaction is a proposed high energy physics experiment at JLAB. A pilot project for jobs at the Open Science fabric is already in place on the OSG Connect infrastructure. The main focus for the collaboration at the moment is to carry out extensive simulations that will further validate the detector design. There is no overall project or data management person for the Moller experiment at this point. The PI is Prof Wouter Deconick Wouter.Deconinck@umanitoba.ca","title":"MOLLER"},{"location":"projects/moller/#moller","text":"The Measurement of a Lepton-Lepton Electroweak Reaction is a proposed high energy physics experiment at JLAB. A pilot project for jobs at the Open Science fabric is already in place on the OSG Connect infrastructure. The main focus for the collaboration at the moment is to carry out extensive simulations that will further validate the detector design. There is no overall project or data management person for the Moller experiment at this point. The PI is Prof Wouter Deconick Wouter.Deconinck@umanitoba.ca","title":"MOLLER"},{"location":"projects/project-list/","text":"List of Supported Projects \u00b6 REDTOP IceCube IGWN LSST-Duke South Pole Telescope VERITAS XENON SNOWMASS21 KOTO CLAS12 GLUEX EIC MOLLER UHE Neutrino Obsevatory Trinity EUSO-SPB2 DUNE","title":"Overview"},{"location":"projects/project-list/#list-of-supported-projects","text":"REDTOP IceCube IGWN LSST-Duke South Pole Telescope VERITAS XENON SNOWMASS21 KOTO CLAS12 GLUEX EIC MOLLER UHE Neutrino Obsevatory Trinity EUSO-SPB2 DUNE","title":"List of Supported Projects"},{"location":"projects/snowmass/","text":"Snowmass21 \u00b6 The Snowmass21 effort is supported with a submit/analysis node on the Connect infrastructure, local and distributed storage, a CVMFS data origin at UChicago, and managed singularity containers for software distribution. The Snowmass effort currently has contributions from more than 50 campuses and institutions across the world with more than 150 users with access to the Snowmass21 submit host with 180 TB of local storage and 50 TB of distributed storage on stash. Data production and analysis lead: John Stupak john.stupak@ou.edu Snowmass has a dedicated operational page found here https://maniaclab.uchicago.edu/snowmass-connect-docs/","title":"Snowmass"},{"location":"projects/snowmass/#snowmass21","text":"The Snowmass21 effort is supported with a submit/analysis node on the Connect infrastructure, local and distributed storage, a CVMFS data origin at UChicago, and managed singularity containers for software distribution. The Snowmass effort currently has contributions from more than 50 campuses and institutions across the world with more than 150 users with access to the Snowmass21 submit host with 180 TB of local storage and 50 TB of distributed storage on stash. Data production and analysis lead: John Stupak john.stupak@ou.edu Snowmass has a dedicated operational page found here https://maniaclab.uchicago.edu/snowmass-connect-docs/","title":"Snowmass21"},{"location":"projects/spt/","text":"South Pole Telescope SPT-3G \u00b6 Support areas for SPT include data management, infrastructure (Submit host, Backups to Federated Facilities, Globus Service, dCache and local storage at UChicago for experimental data from the South Pole and production-quality data from OSG), and best practices for software and job management, SPT is a collaboration that involves over 30 University campuses in the US along with National Laboratories. Examples of main contributions come from the University of Chicago, University of California Berkeley and Davis, the University of Illinois at Urbana/Champaign, Michigan State, and Argonne and Fermi National Labs. SPT-3G is supported by the NSF award # 1852617 (South Pole Telescope Operations and Data Products). Data production and analysis Lead: Sasha Rahlin arahlin@fnal.gov Data management coordination: Tom Crawford tcrawfor@kicp.uchicago.edu Account Portal and sign up for Access to Resources SPT Local Storage Dashboard SPT Grid Storage Dashboard Information fo Users \u00b6 Guide to access OSG resources Guid to obtain Grid Certificates General Information on Software availability and documenation \u00b6","title":"South Pole Telescope"},{"location":"projects/spt/#south-pole-telescope-spt-3g","text":"Support areas for SPT include data management, infrastructure (Submit host, Backups to Federated Facilities, Globus Service, dCache and local storage at UChicago for experimental data from the South Pole and production-quality data from OSG), and best practices for software and job management, SPT is a collaboration that involves over 30 University campuses in the US along with National Laboratories. Examples of main contributions come from the University of Chicago, University of California Berkeley and Davis, the University of Illinois at Urbana/Champaign, Michigan State, and Argonne and Fermi National Labs. SPT-3G is supported by the NSF award # 1852617 (South Pole Telescope Operations and Data Products). Data production and analysis Lead: Sasha Rahlin arahlin@fnal.gov Data management coordination: Tom Crawford tcrawfor@kicp.uchicago.edu Account Portal and sign up for Access to Resources SPT Local Storage Dashboard SPT Grid Storage Dashboard","title":"South Pole Telescope SPT-3G"},{"location":"projects/spt/#information-fo-users","text":"Guide to access OSG resources Guid to obtain Grid Certificates General Information on Software availability and documenation","title":"Information fo Users"},{"location":"projects/spt/#_1","text":"","title":""},{"location":"projects/veritas/","text":"VERITAS Collaboration \u00b6","title":"VERITAS"},{"location":"projects/veritas/#veritas-collaboration","text":"","title":"VERITAS Collaboration"},{"location":"projects/xenon/","text":"XENON \u00b6 Support areas for XENON include data management, infrastructure (Submit host, dCache storage, FTS service for Rucio managed third-party transfers, Rucio Service), and best practices for software and job management. XENON is supported by NSF institutional grants PHY-2112796 at UChicago, PHY-2112851 at Columbia University, PHY-2112801 at Rice University, PHY-2112802 at UC San Diego and PHY-21128-3 at Purdue University. Data Production team Lead: Evan Shockley eshockley@physics.ucsd.edu MC Production team Lead: Diego Ramirez diego.ramirez@physik.uni-freiburg.de At present, XENON consumes ~6 PB of distributed storage f in the US & Europe managed by Rucio. Total storage utilization has remained constant in time, as the collaboration is deleting older data - from XENON 1T - while it continues to gather fresh data from the current phase - XENON nT. Nonetheless, the overall replicated distributed storage is expected to increase by the end of the 5-year lifetime of the experiment because the amount of data volume for the nT phase is projected to be five times larger than the 1T volume. Status of Services \u00b6 Rucio Server Ganglia FTS Server Ganglia Check-mk Xenon Rucio Group Rucio Postgress XENON login node Weekly XENON analysis/Compute call \u00b6 URL: https://riceuniversity.zoom.us/j/4815153031 Minutes:https://xe1t-wiki.lngs.infn.it/doku.php?id=xenon:xenon1t:analysistools:200402","title":"XENON"},{"location":"projects/xenon/#xenon","text":"Support areas for XENON include data management, infrastructure (Submit host, dCache storage, FTS service for Rucio managed third-party transfers, Rucio Service), and best practices for software and job management. XENON is supported by NSF institutional grants PHY-2112796 at UChicago, PHY-2112851 at Columbia University, PHY-2112801 at Rice University, PHY-2112802 at UC San Diego and PHY-21128-3 at Purdue University. Data Production team Lead: Evan Shockley eshockley@physics.ucsd.edu MC Production team Lead: Diego Ramirez diego.ramirez@physik.uni-freiburg.de At present, XENON consumes ~6 PB of distributed storage f in the US & Europe managed by Rucio. Total storage utilization has remained constant in time, as the collaboration is deleting older data - from XENON 1T - while it continues to gather fresh data from the current phase - XENON nT. Nonetheless, the overall replicated distributed storage is expected to increase by the end of the 5-year lifetime of the experiment because the amount of data volume for the nT phase is projected to be five times larger than the 1T volume.","title":"XENON"},{"location":"projects/xenon/#status-of-services","text":"Rucio Server Ganglia FTS Server Ganglia Check-mk Xenon Rucio Group Rucio Postgress XENON login node","title":"Status of Services"},{"location":"projects/xenon/#weekly-xenon-analysiscompute-call","text":"URL: https://riceuniversity.zoom.us/j/4815153031 Minutes:https://xe1t-wiki.lngs.infn.it/doku.php?id=xenon:xenon1t:analysistools:200402","title":"Weekly XENON analysis/Compute call"}]}